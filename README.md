# ğŸ“Š Uplift Modeling for Marketing Campaign Optimization

A machine learning project that predicts the **causal impact** of marketing interventions at the individual customer level, enabling data-driven targeting decisions that maximize ROI.

---

## ğŸ¯ Business Problem

**Challenge:** Random email campaigns waste resources on customers who would:
- Buy anyway (without emails)
- Never buy (unresponsive)
- Be discouraged by emails (sleeping dogs)

**Solution:** Uplift modeling identifies customers who will **only** convert **because** of the email, maximizing campaign efficiency.

---

## ğŸ“ˆ Key Results

| Metric | Random Targeting | Smart Targeting | Improvement |
|--------|-----------------|-----------------|-------------|
| Conversion Uplift | 0.63% | 1.41% | **+123%** |
| Customers Targeted | 8,523 | 2,557 | **-70%** |
| Cost Efficiency | Baseline | 3.3x | **+230%** |

**ROI Impact:** Same results with 70% fewer emails = 3.3x cost reduction

---

## ğŸ› ï¸ Technical Approach

### 1. **Methodology: T-Learner**
- **Two separate models:** One for treated group, one for control
- **Uplift calculation:** Ï„(x) = P(Y=1|T=1,X) - P(Y=1|T=0,X)
- **Advantages:** Simple, interpretable, handles heterogeneous effects

### 2. **Model Architecture**
```python
Control Model (XGBoost) â†’ P(conversion | no email)
Treatment Model (XGBoost) â†’ P(conversion | email)
                              â†“
                    Uplift = Difference
```

### 3. **Evaluation Framework**
- **Decile Analysis:** Customers ranked into 10 segments
- **Bootstrap Confidence Intervals:** 1000 resamples for statistical validity
- **Money Chart:** Visual validation of model quality

---

## ğŸ“‚ Project Structure

```
uplift-modeling/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ email_marketing_campaign.csv    # Historical A/B test data
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ eda_uplift.ipynb               # Full analysis pipeline
â”‚
â”œâ”€â”€ assets/
â”‚   â””â”€â”€ uplift_with_ci.png             # Key visualization
â”‚
â”œâ”€â”€ models/                             # (Future: Saved models)
â”‚
â””â”€â”€ README.md
```

---

## ğŸš€ Quick Start

### Prerequisites
```bash
pip install pandas numpy scikit-learn xgboost matplotlib seaborn scipy
```

### Run the Analysis
```python
# Load and prepare data
df = pd.read_csv('data/email_marketing_campaign.csv')

# Train T-Learner models
model_control.fit(X_train_control, y_train_control)
model_treatment.fit(X_train_treatment, y_train_treatment)

# Predict uplift
uplift = model_treatment.predict_proba(X_test) - model_control.predict_proba(X_test)

# Evaluate with confidence intervals
bootstrap_confidence_intervals(df_results)
```

---

## ğŸ“Š Key Visualizations

### Uplift by Customer Segment
![Uplift with Confidence Intervals](assets/uplift_with_ci.png)

**Interpretation:**
- **Deciles 3, 5, 6:** Significant positive uplift (target these!)
- **Error bars:** Show statistical uncertainty
- **Red line:** Random targeting baseline

**Business Action:** Focus campaigns on ~30% of customers for 2x better results

---

## ğŸ”¬ Model Performance

### Statistical Validation
```
âœ… Significant Uplift Found: 3/10 deciles
ğŸ“ Average CI Width: 0.0248 (narrow = reliable)
ğŸ¯ Best Segment: Decile 5 (2.45% uplift)
```

### Confidence Intervals (95%)
| Decile | Uplift | CI Low | CI High | Significant |
|--------|--------|--------|---------|-------------|
| 5      | 2.45%  | 0.92%  | 4.07%   | âœ… Yes      |
| 3      | 1.55%  | 0.15%  | 3.13%   | âœ… Yes      |
| 6      | 0.25%  | 0.00%  | 0.76%   | âœ… Yes      |

---

## ğŸ’¡ Business Insights

### Actionable Recommendations

1. **Immediate Action:**
   - Target Deciles 3, 5, 6 only (2,557 customers)
   - Reduce email volume by 70%
   - Expected conversion lift: +123%

2. **ROI Calculation:**
   ```
   Cost Savings = 70% reduction Ã— $0.10/email Ã— 8,523 customers
                = $596 per campaign
   
   Revenue Increase = 1.41% uplift Ã— 2,557 customers Ã— $50 AOV
                    = $1,803 per campaign
   
   Net Improvement = $2,399 per campaign
   ```

3. **Customer Segments:**
   - **Persuadables (30%):** High uplift - prioritize
   - **Sure Things (30%):** Will buy anyway - reduce contact
   - **Lost Causes (40%):** Unresponsive - exclude

---

## ğŸ”„ Future Enhancements

### Phase 1: Model Improvements
- [ ] Implement **X-Learner** for better performance
- [ ] Add **Qini curves** for lift validation
- [ ] Compare with **Causal Forest** algorithms
- [ ] Hyperparameter tuning with Optuna

### Phase 2: Feature Engineering
- [ ] RFM (Recency, Frequency, Monetary) features
- [ ] Customer lifetime value predictions
- [ ] Interaction terms (e.g., history Ã— channel)
- [ ] Behavioral clustering

### Phase 3: Production Pipeline
- [ ] Model serialization (joblib/pickle)
- [ ] REST API for real-time scoring
- [ ] A/B testing framework
- [ ] Monitoring dashboards

### Phase 4: Advanced Analytics
- [ ] Multi-treatment uplift modeling
- [ ] Cost-sensitive learning
- [ ] Time-series cross-validation
- [ ] Fairness/bias analysis

---

## ğŸ“š Technical Deep Dive

### Why T-Learner?

**Pros:**
- âœ… Simple to implement and explain
- âœ… Handles treatment heterogeneity
- âœ… No assumptions about treatment effect form

**Cons:**
- âš ï¸ Requires large sample sizes
- âš ï¸ Two models to maintain
- âš ï¸ Can be unstable with noisy data

**When to Use:**
- Large datasets (>10K samples)
- Clear control/treatment split
- Exploratory analysis phase

### Alternative Approaches

| Method | Best For | Complexity |
|--------|----------|------------|
| **S-Learner** | Small datasets | Low |
| **T-Learner** | Standard use case | Medium |
| **X-Learner** | Imbalanced groups | High |
| **Causal Forest** | Non-linear effects | Very High |

---

## ğŸ§ª Validation Strategy

### 1. **Statistical Tests**
```python
# Bootstrap confidence intervals (1000 iterations)
uplift_mean, ci_low, ci_high = bootstrap_uplift(group, n_bootstrap=1000)

# Reject if CI includes zero
significant = not (ci_low < 0 and ci_high > 0)
```

### 2. **Business Validation**
- Compare predicted uplift vs. actual A/B test results
- Track campaign performance over time
- Conduct periodic holdout tests

### 3. **Robustness Checks**
- Cross-validation (5-fold)
- Temporal validation (train on old data, test on new)
- Sensitivity analysis (vary treatment assignment)

---

## ğŸ“– References & Learning Resources

### Papers
1. [Athey & Imbens (2016): Recursive Partitioning for Heterogeneous Causal Effects](https://arxiv.org/abs/1504.01132)
2. [KÃ¼nzel et al. (2019): Metalearners for Estimating Heterogeneous Treatment Effects](https://arxiv.org/abs/1706.03461)

### Libraries
- **CausalML** (Uber): Production-ready uplift modeling
- **EconML** (Microsoft): Advanced causal inference
- **DoWhy** (Microsoft): Causal reasoning framework

### Books
- *Causal Inference for Statistics* (Pearl, 2016)
- *Mostly Harmless Econometrics* (Angrist & Pischke, 2009)

---

## ğŸ¤ Contributing

This project demonstrates foundational uplift modeling skills. Suggested improvements:

1. **Code Quality:**
   - Add type hints and docstrings
   - Create modular functions
   - Write unit tests

2. **Reproducibility:**
   - Set random seeds
   - Version control datasets
   - Document environment

3. **Scalability:**
   - Parallelize bootstrap
   - Use Dask for large datasets
   - Optimize XGBoost parameters

---

## ğŸ“ License

MIT License - Feel free to use this project as a template for your own uplift modeling work.

---

## ğŸ“§ Contact

**Author:** [Your Name]  
**Email:** your.email@example.com  
**LinkedIn:** linkedin.com/in/yourprofile  
**GitHub:** github.com/yourusername

---

## ğŸ† Skills Demonstrated

- âœ… Causal inference (T-Learner)
- âœ… Statistical validation (bootstrap CI)
- âœ… Business metrics translation
- âœ… Data visualization
- âœ… XGBoost modeling
- âœ… ROI analysis
- âœ… A/B testing principles

**Suitable for:** Junior Data Scientist, ML Engineer, Marketing Analyst roles

---

*Built with Python, XGBoost, and a passion for data-driven decision making* ğŸš€